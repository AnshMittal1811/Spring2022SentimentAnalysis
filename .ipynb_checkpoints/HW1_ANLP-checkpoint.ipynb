{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW1_ANLP_AnmolSajnani_2581081096.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qdkjCpHgNjI",
        "outputId": "527b8bf6-cebc-4edf-e999-765048fd56cb"
      },
      "source": [
        "!python version;\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file 'version': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xJsrLd9jcw0",
        "outputId": "928a0393-d900-4e06-ce97-a34149cb6cf8"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "!pip install contractions\n",
        "import contractions\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.0.52)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.21)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-iIHrhtjieq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3c243ef-4684-4a48-db20-9112d65104f1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FM59-IDor3zh",
        "outputId": "bf1bbd92-a376-4301-fd73-9051f6ce73d4"
      },
      "source": [
        "df= pd.read_csv(\"/content/drive/MyDrive/NLP/ANLP_HW1_Dataset.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (1,4,7,8,9,15,16,17,18,19,20,21,22,23,24,25,26,27,28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpvVbejrtUGU"
      },
      "source": [
        "####ONLY KEEPING'REVIEW BODY' AND 'STAR_RATING' AS DEFINED IN THE PROBLEM STATEMENT######################\n",
        "dataset = df[['star_rating','review_body']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ty7ANEcqPv0K",
        "outputId": "5b437273-ab07-4e7e-dc0d-292d203d5c15"
      },
      "source": [
        "######################################DISPLAYING STATISTICS OF ALL THREE CLASSES##################################\n",
        "Positive_Reviews = dataset[dataset.star_rating ==4].shape[0] + dataset[dataset.star_rating ==5].shape[0]\n",
        "Negative_Reviews =  dataset[dataset.star_rating ==1].shape[0] + dataset[dataset.star_rating ==2].shape[0]\n",
        "Neutral_Reviews = dataset[dataset.star_rating ==3].shape[0]\n",
        "print('Statistics- Positive Reviews:', Positive_Reviews , ', Negative Reviews:', Negative_Reviews, ', Neutral_Reviews:', Neutral_Reviews)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statistics- Positive Reviews: 733804 , Negative Reviews: 107682 , Neutral_Reviews: 60259\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfbRbGF6tUZV"
      },
      "source": [
        "##################DISCARDING THE REVIEWS WHICH HAVE 'STAR_RATING' AS 3 I.E NEUTRAL REVIEWS###################\n",
        "dataset = dataset[dataset.star_rating != 3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ox9231FBtlGg"
      },
      "source": [
        "#####################ASSIGNING LABELS TO THE DATA; 1 CORRESPONDS TO RATINGS {4,5}, 0 CORRESPONDS TO RATINGS {1,2}#############\n",
        "dataset['star_rating'] = pd.to_numeric(dataset['star_rating'],errors='coerce')\n",
        "dataset['labels'] = np.where(dataset['star_rating']>=4, 1, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-KhNNumtliE"
      },
      "source": [
        "#######################DIVIDING POSITIVE & NEGATIVE REVIEWS AND SELECTING A RANDOM SAMPLE OF 10000 DATAROWS###############################\n",
        "dataset_pos= dataset.loc[dataset['labels'] == 1]\n",
        "dataset_pos= dataset.sample(100000)\n",
        "dataset_neg= dataset.loc[dataset['labels'] == 0]\n",
        "dataset_neg = dataset.sample(100000)\n",
        "dataset= dataset_pos\n",
        "dataset= dataset.append(dataset_neg)\n",
        "dataset = dataset.sample(frac = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuGHBf1wFZJl",
        "outputId": "4d62699d-175a-4335-88bc-9bd67852ae62"
      },
      "source": [
        "####################DISPLAYING AVERAGE LENGTH OF REVIEWS BEFORE DATA CLEANING###################\n",
        "Before_DataCleaning = dataset['review_body'].astype(str).apply(lambda x : len(str(x))).mean()\n",
        "print('Average Length of Reviews before Data Cleaning',Before_DataCleaning)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Length of Reviews before Data Cleaning 187.05533\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXx15onxt_fT"
      },
      "source": [
        "#######################CONVERT ALL REVIEWS INTO LOWER CASE###########\n",
        "dataset['review_body']= dataset['review_body'].apply(lambda txt: str(txt).lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6kdppSSt_oc"
      },
      "source": [
        "##########################REMOVE HTML and URLs FROM CUSTOMER REVIEWS#################\n",
        "###########1. REMOVING URL's####################\n",
        "dataset['review_body']= dataset['review_body'].apply(lambda txt: re.sub(r\"http\\S+\", \"\", txt) )\n",
        "##########2. REMOVING HTML####################\n",
        "dataset['review_body']= dataset['review_body'].apply(lambda txt: BeautifulSoup(txt, 'lxml').get_text())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Kh6Iufut_p-"
      },
      "source": [
        "##########################REMOVE NON-ALPHABETICAL CHARACTERS#######################\n",
        "dataset['review_body']= dataset['review_body'].apply(lambda txt: re.sub(\"[^a-zA-Z]+\", \" \",txt) ) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yE1kKjfyt_vG"
      },
      "source": [
        "#########################REMOVE THE EXTRA SPACES BETWEEN WORDS###################\n",
        "dataset['review_body']= dataset['review_body'].apply(lambda txt: re.sub(\" +\",\" \", txt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkwrGbRUuZnA"
      },
      "source": [
        "########################PERFORM CONTRACTIONS#######################################\n",
        "import contractions\n",
        "dataset['review_body']= dataset['review_body'].apply(lambda txt: contractions.fix(txt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4D-2CQMiUhB_",
        "outputId": "a33d0cf3-1ad2-4e10-cf0f-42e27c5b42fc"
      },
      "source": [
        "######################DISPLAYING AVERAGE LENGTH OF REVIEWS AFTER DATA CLEANING & BEFORE PRE PROCESSING#############\n",
        "AFter_DataCleaning = dataset['review_body'].apply(len).mean()\n",
        "print('Average Length of Reviews after Data Cleaning',AFter_DataCleaning)\n",
        "print('Average Length of Reviews before Pre-Processing',AFter_DataCleaning)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Length of Reviews after Data Cleaning 179.510425\n",
            "Average Length of Reviews before Pre-Processing 179.510425\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uhZtzknuZt7",
        "outputId": "2306ffbe-982c-44b6-ac48-8d09caa19090"
      },
      "source": [
        "#####################REMOVE STOPWORDS##########################################\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "def Remove_Stopwords(message):\n",
        "  tokens = message.split(\" \")\n",
        "  stop_words_list = set (stopwords.words('english'))\n",
        "  clean_message = [word for word in tokens if not word in stop_words_list]\n",
        "  return [(\" \").join(clean_message)]\n",
        "  \n",
        "dataset['review_body']= dataset['review_body'].apply(lambda txt: Remove_Stopwords(txt))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMNiWSoP1ahB"
      },
      "source": [
        "######################################PERFORM LEMMATIZATION#############################\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "def lemmatize(message):\n",
        "    word_lemma = WordNetLemmatizer()\n",
        "    lemmatize_word= [word_lemma.lemmatize(word) for word in message]\n",
        "    return (\" \").join(lemmatize_word)\n",
        "dataset['review_body']= dataset['review_body'].apply(lambda txt: lemmatize(txt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOTOgU-PSukd",
        "outputId": "4a87a817-0555-41ea-ec0e-698a9a091068"
      },
      "source": [
        "######################DISPLAYING AVERAGE LENGTH OF REVIEWS AFTER PRE PROCESSING#############\n",
        "AFter_Preprocessing = dataset['review_body'].apply(len).mean()\n",
        "print('Average Length of Reviews after Pre-Processing',AFter_Preprocessing)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Length of Reviews after Pre-Processing 113.981\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzfkIo3j4PuP"
      },
      "source": [
        "############################################FEATURE EXTRACTION & SPLITTING DATA FOR TRAINING & TESTING######################\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "data_train, data_test, y_train, y_test = train_test_split(dataset['review_body'], dataset['labels'], test_size = 0.2)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "Training_data = tfidf.fit_transform(data_train)\n",
        "Testing_data = tfidf.transform(data_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQlxBhs04gvx",
        "outputId": "d9f7e4b4-5fc7-4361-dae1-370790616c15"
      },
      "source": [
        "######################################PERCEPTRON MODEL##################################\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "Perceptron_Model= Perceptron()\n",
        "Perceptron_Model.fit(Training_data,y_train)\n",
        "Prediction = Perceptron_Model.predict(Testing_data)\n",
        "print('Perceptron Model(Testing_data)--Accuracy:',accuracy_score(y_test,Prediction))\n",
        "print('Perceptron Model(Testing_data)--Precision Score:',precision_score(y_test,Prediction))\n",
        "print('Perceptron Model(Testing_data)--Recall Score:',recall_score(y_test,Prediction))\n",
        "print('Perceptron Model(Testing_data)--F1 Score:',f1_score(y_test,Prediction))\n",
        "Prediction = Perceptron_Model.predict(Training_data)\n",
        "print('Perceptron Model(Training_data)--Accuracy:',accuracy_score(y_train,Prediction))\n",
        "print('Perceptron Model(Training_data)--Precision Score:',precision_score(y_train,Prediction))\n",
        "print('Perceptron Model(Training_data)--Recall Score:',recall_score(y_train,Prediction))\n",
        "print('Perceptron Model(Training_data)--F1 Score:',f1_score(y_train,Prediction))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perceptron Model(Testing_data)--Accuracy: 0.919625\n",
            "Perceptron Model(Testing_data)--Precision Score: 0.9516885958598451\n",
            "Perceptron Model(Testing_data)--Recall Score: 0.9555677613320999\n",
            "Perceptron Model(Testing_data)--F1 Score: 0.9536242336819329\n",
            "Perceptron Model(Training_data)--Accuracy: 0.9480125\n",
            "Perceptron Model(Training_data)--Precision Score: 0.9686538225902942\n",
            "Perceptron Model(Training_data)--Recall Score: 0.9713141709007596\n",
            "Perceptron Model(Training_data)--F1 Score: 0.9699821726295733\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vci07v068NWg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d57e4e60-29e0-4008-9a71-247fb7dbd370"
      },
      "source": [
        "############################################SUPPORT VECTOR MACHINE#######################\n",
        "from sklearn.svm import SVC\n",
        "svm_Model = SVC(kernel='linear',max_iter=500)\n",
        "svm_Model.fit(Training_data,y_train)\n",
        "prediction = svm_Model.predict(Testing_data)\n",
        "print('SVM Model(Testing_data)--Accuracy:',accuracy_score(y_test,prediction))\n",
        "print('SVM Model(Testing_data)--Precision Score:',precision_score(y_test,prediction))\n",
        "print('SVM Model(Testing_data)--Recall Score:',recall_score(y_test,prediction))\n",
        "print('SVM Model(Testing_data)--F1 Score:',f1_score(y_test,prediction))\n",
        "prediction = svm_Model.predict(Training_data)\n",
        "print('SVM Model(Training_data)--Accuracy:',accuracy_score(y_train,prediction))\n",
        "print('SVM Model(Training_data)--Precision Score:',precision_score(y_train,prediction))\n",
        "print('SVM Model(Training_data)--Recall Score:',recall_score(y_train,prediction))\n",
        "print('SVM Model(Training_data)--F1 Score:',f1_score(y_train,prediction))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Model(Testing_data)--Accuracy: 0.850925\n",
            "SVM Model(Testing_data)--Precision Score: 0.900708227192565\n",
            "SVM Model(Testing_data)--Recall Score: 0.9301572617946346\n",
            "SVM Model(Testing_data)--F1 Score: 0.9151959041456303\n",
            "SVM Model(Training_data)--Accuracy: 0.8515875\n",
            "SVM Model(Training_data)--Precision Score: 0.9015295362345243\n",
            "SVM Model(Training_data)--Recall Score: 0.9299513591257652\n",
            "SVM Model(Training_data)--F1 Score: 0.915519915754721\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqRBwYIcMzTT",
        "outputId": "8a5fb1b8-a417-4803-aa0e-6757402f3a0c"
      },
      "source": [
        "#########################################LOGISTIC REGRESSION##############################\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg_Model = LogisticRegression(max_iter = 500)\n",
        "logreg_Model.fit(Training_data, y_train)\n",
        "prediction = logreg_Model.predict(Testing_data)\n",
        "print('Logistic_Regression(Testing_data) Model--Accuracy:',accuracy_score(y_test,prediction))\n",
        "print('Logistic_Regression(Testing_data) Model--Precision Score:',precision_score(y_test,prediction))\n",
        "print('Logistic_Regression(Testing_data) Model--Recall Score:',recall_score(y_test,prediction))\n",
        "print('Logistic_Regression(Testing_data) Model--F1 Score:',f1_score(y_test,prediction))\n",
        "prediction = logreg_Model.predict(Training_data)\n",
        "print('Logistic_Regression(Training_data) Model--Accuracy:',accuracy_score(y_train,prediction))\n",
        "print('Logistic_Regression(Training_data) Model--Precision Score:',precision_score(y_train,prediction))\n",
        "print('Logistic_Regression(Training_data) Model--Recall Score:',recall_score(y_train,prediction))\n",
        "print('Logistic_Regression(Training_data) Model--F1 Score:',f1_score(y_train,prediction))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic_Regression(Testing_data) Model--Accuracy: 0.9397\n",
            "Logistic_Regression(Testing_data) Model--Precision Score: 0.9481394830659536\n",
            "Logistic_Regression(Testing_data) Model--Recall Score: 0.9841003700277521\n",
            "Logistic_Regression(Testing_data) Model--F1 Score: 0.9657852927825692\n",
            "Logistic_Regression(Training_data) Model--Accuracy: 0.94645\n",
            "Logistic_Regression(Training_data) Model--Precision Score: 0.9527736498036015\n",
            "Logistic_Regression(Training_data) Model--Recall Score: 0.9869977811666583\n",
            "Logistic_Regression(Training_data) Model--F1 Score: 0.9695838007469151\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Hs85aCuNmoy",
        "outputId": "734dedb2-1d74-4b30-8479-c09d2eca11a7"
      },
      "source": [
        "######################################MULTINOMINAL REGRESSION#############################\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "MulNb_Model = MultinomialNB(alpha=6.5, fit_prior=False)\n",
        "MulNb_Model.fit(Training_data,y_train)\n",
        "prediction = MulNb_Model.predict(Testing_data)\n",
        "print('Multinominal_Naive_Bayes(Testing_data) Model--Accuracy:',accuracy_score(y_test,prediction))\n",
        "print('Multinominal_Naive_Bayes(Testing_data) Model--Precision Score:',precision_score(y_test,prediction))\n",
        "print('Multinominal_Naive_Bayes(Testing_data) Model--Recall Score:',recall_score(y_test,prediction))\n",
        "print('Multinominal_Naive_Bayes(Testing_data) Model--F1 Score:',f1_score(y_test,prediction))\n",
        "\n",
        "prediction = MulNb_Model.predict(Training_data)\n",
        "print('Multinominal_Naive_Bayes(Training_data) Model--Accuracy:',accuracy_score(y_train,prediction))\n",
        "print('Multinominal_Naive_Bayes(Training_data) Model--Precision Score:',precision_score(y_train,prediction))\n",
        "print('Multinominal_Naive_Bayes(Training_data) Model--Recall Score:',recall_score(y_train,prediction))\n",
        "print('Multinominal_Naive_Bayes(Training_data) Model--F1 Score:',f1_score(y_train,prediction))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multinominal_Naive_Bayes(Testing_data) Model--Accuracy: 0.8843\n",
            "Multinominal_Naive_Bayes(Testing_data) Model--Precision Score: 0.887351983039454\n",
            "Multinominal_Naive_Bayes(Testing_data) Model--Recall Score: 0.9921658186864015\n",
            "Multinominal_Naive_Bayes(Testing_data) Model--F1 Score: 0.9368363586733999\n",
            "Multinominal_Naive_Bayes(Training_data) Model--Accuracy: 0.886825\n",
            "Multinominal_Naive_Bayes(Training_data) Model--Precision Score: 0.8899077214393639\n",
            "Multinominal_Naive_Bayes(Training_data) Model--Recall Score: 0.9918257312392943\n",
            "Multinominal_Naive_Bayes(Training_data) Model--F1 Score: 0.9381066965628503\n"
          ]
        }
      ]
    }
  ]
}