{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import unidecode\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet, stopwords, words\n",
    "import re\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from nltk.metrics.distance  import edit_distance\n",
    "import emoji\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = open('./Data/result.json', 'r', encoding='utf8')\n",
    "data = json.load(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['messages'][10000].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_extraction(data):\n",
    "    df = []\n",
    "    crypto = ['shib', 'doge', 'shiba', 'dogecoin']\n",
    "    print(\"Extracting Messages...\")\n",
    "    for i in tqdm(range(len(data['messages']))):\n",
    "        frame = []\n",
    "        st = data['messages'][i]\n",
    "        \n",
    "        if type(st['text']) == str:\n",
    "            if any(curr in st['text'].lower() for curr in crypto):\n",
    "                frame.append(st['date'].split(\"T\")[0])\n",
    "                frame.append(st['text'])\n",
    "\n",
    "        if type(st['text']) == list: \n",
    "            b = str()\n",
    "            \n",
    "            for j in st['text']: \n",
    "                if type(j) == dict:\n",
    "                    a = j['text']\n",
    "                    b += a\n",
    "                else: \n",
    "                    b += j\n",
    "                    \n",
    "            if any(curr in b.lower() for curr in crypto):\n",
    "                frame.append(st['date'].split(\"T\")[0])\n",
    "                frame.append(b)\n",
    "        \n",
    "        if len(frame): \n",
    "            df.append(frame)\n",
    "        \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Messages...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 49436/49436 [00:00<00:00, 309742.40it/s]\n"
     ]
    }
   ],
   "source": [
    "messages = dataset_extraction(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_df(messages): \n",
    "    df = pd.DataFrame(messages, columns = ['Date', 'Messages'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = convert_data_to_df(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-05-01</td>\n",
       "      <td>Doge is going craY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-05-01</td>\n",
       "      <td>Sell target of doge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-05-01</td>\n",
       "      <td>Doge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-05-01</td>\n",
       "      <td>Dogecoin!!! Que hago?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-05-01</td>\n",
       "      <td>Anyway, is doge a good crypto for long term in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3056</th>\n",
       "      <td>2021-05-15</td>\n",
       "      <td>How is Shib?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3057</th>\n",
       "      <td>2021-05-15</td>\n",
       "      <td>How is shib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3058</th>\n",
       "      <td>2021-05-15</td>\n",
       "      <td>Shiba swap is gonna launch soon... It will def...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3059</th>\n",
       "      <td>2021-05-15</td>\n",
       "      <td>Whole crypto market is being dominated by bear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3060</th>\n",
       "      <td>2021-05-15</td>\n",
       "      <td>Any one belives in shiba?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3061 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date                                           Messages\n",
       "0     2021-05-01                                 Doge is going craY\n",
       "1     2021-05-01                                Sell target of doge\n",
       "2     2021-05-01                                               Doge\n",
       "3     2021-05-01                              Dogecoin!!! Que hago?\n",
       "4     2021-05-01  Anyway, is doge a good crypto for long term in...\n",
       "...          ...                                                ...\n",
       "3056  2021-05-15                                       How is Shib?\n",
       "3057  2021-05-15                                        How is shib\n",
       "3058  2021-05-15  Shiba swap is gonna launch soon... It will def...\n",
       "3059  2021-05-15  Whole crypto market is being dominated by bear...\n",
       "3060  2021-05-15                          Any one belives in shiba?\n",
       "\n",
       "[3061 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    df = demojizing(df)\n",
    "    df = convert_accented_chars(df)\n",
    "    df = remove_case_sensitive(df)\n",
    "    df = remove_htmls_and_urls(df)\n",
    "    df = remove_extra_spaces_between_words(df)\n",
    "    df = expand_contractions(df)\n",
    "    df = remove_stop_words(df)\n",
    "    df = lemmatization(df)\n",
    "    df = removing_non_english(df)\n",
    "    df = spelling_corrections(df)\n",
    "    return df\n",
    "\n",
    "def demojizing(dataframe): \n",
    "    tqdm.pandas()\n",
    "    print(\"Decoding Emojis in Text...\")\n",
    "    dataframe[\"Messages\"] = dataframe[\"Messages\"].progress_apply(lambda txt: emoji.demojize(txt))\n",
    "    return dataframe\n",
    "\n",
    "def convert_accented_chars(dataframe): \n",
    "    tqdm.pandas()\n",
    "    print(\"Converting Accented Characters...\")\n",
    "    dataframe[\"Messages\"] = dataframe[\"Messages\"].progress_apply(lambda txt: unidecode.unidecode(txt))\n",
    "    return dataframe\n",
    "\n",
    "def remove_case_sensitive(dataframe):\n",
    "    tqdm.pandas()\n",
    "    print(\"Removing Case Sensitive Characters...\")\n",
    "    dataframe['Messages'] = dataframe['Messages'].progress_apply(lambda txt: str(txt).lower())\n",
    "    return dataframe\n",
    "\n",
    "def remove_htmls_and_urls(dataframe):\n",
    "    tqdm.pandas()\n",
    "    print(\"Removing HTMLs and URLs...\")\n",
    "    dataframe['Messages'] = dataframe['Messages'].progress_apply(lambda txt: re.sub(r\"http\\S+\", \"\", txt))\n",
    "    dataframe['Messages'] = dataframe['Messages'].progress_apply(lambda txt: BeautifulSoup(txt, 'lxml').get_text())\n",
    "    return dataframe\n",
    "    \n",
    "def remove_extra_spaces_between_words(dataframe):\n",
    "    tqdm.pandas()\n",
    "    print(\"Removing Extra Whitespaces...\")\n",
    "    dataframe['Messages'] = dataframe['Messages'].progress_apply(lambda txt: re.sub(\" +\",\" \", txt))\n",
    "    return dataframe\n",
    "\n",
    "def expand_contractions(dataframe):\n",
    "    tqdm.pandas()\n",
    "    print(\"Expanding Contractions...\")\n",
    "    dataframe['Messages'] = dataframe['Messages'].progress_apply(lambda txt: contractions.fix(txt))\n",
    "    return dataframe\n",
    "\n",
    "def spelling_corrections(dataframe):\n",
    "    \n",
    "    def spell_check(message, correct_words):\n",
    "        new_message = \"\"\n",
    "        for word in message.split(\" \"):\n",
    "            if word.isalpha() and (word not in correct_words) and (word.lower() not in [\"doge\", \"dogecoin\", \"shibe\", \"shiba\", \"shib\", \"shiba inu\"]):\n",
    "                temp = [(edit_distance(word, w),w) for w in correct_words if w[0]==word[0]]\n",
    "                new_message = new_message + sorted(temp, key = lambda val:val[0])[0][1] + \" \"\n",
    "            else:\n",
    "                new_message =  new_message + word + \" \"\n",
    "        return new_message\n",
    "\n",
    "    tqdm.pandas()\n",
    "    print(\"Performing Spelling Corrections...\")\n",
    "    \n",
    "    slangs = [\"doge\", \"dogecoin\", \"dogecoins\", \"shib\", \"shiba\", \"shiba inu\", \"shibe inu\", \n",
    "          \"dollar\", \"dolar\", \"$\", \"ps\", \"p.s.\", \"app\", \"money\", \"tarde\", \"telegram\", \"whatsapp\", \n",
    "          \"buy\", \"issue\", \"crypto\", \"usdc\", \"bank\", \"account\", \"portfolio\", \"Elon\", \"Musk\", \"shibaa\",\n",
    "          \"profit\", \"cro\", \"€\", \"inr\", \"mill\", \"cdc\", \"tbh\", \"hi\", \"hey\", \"plz\", \"wbu\", \"%\",\n",
    "          \"crypto.com\", \"email\", \"usdt\", \"cent\", \"ct\", \"mil\", \"ppl\", \"btc\", \"curr\"]\n",
    "\n",
    "    a = [w for w in wordnet.all_lemma_names()]\n",
    "    a = list(set(a).union(set(slangs)))\n",
    "    correct_words = list(set(words.words()).union(set(a)))    \n",
    "    dataframe['Messages'] = dataframe['Messages'].progress_apply(lambda txt: spell_check(txt, correct_words))\n",
    "    return dataframe\n",
    "\n",
    "def removing_non_english(dataframe): \n",
    "    def word_in_english(message, correct_words): \n",
    "        st = []\n",
    "        for word in message.split(\" \"): \n",
    "            if wordnet.synsets(word) or word.lower() in correct_words:\n",
    "                st.append(1)\n",
    "            else: \n",
    "                st.append(0)\n",
    "\n",
    "        cutoff_value = 0.45\n",
    "        \n",
    "        if sum(st)/len(st) >= cutoff_value: \n",
    "            return message\n",
    "        else: \n",
    "            return \"\"\n",
    "        \n",
    "    tqdm.pandas()\n",
    "    print(\"Checking English Messages...\")\n",
    "    slangs = [\"doge\", \"dogecoin\", \"dogecoins\", \"shib\", \"shiba\", \"shiba inu\", \"shibe inu\", \n",
    "              \"dollar\", \"dolar\", \"$\", \"ps\", \"p.s.\", \"app\", \"money\", \"tarde\", \"telegram\", \"whatsapp\", \n",
    "              \"buy\", \"issue\", \"crypto\", \"usdc\", \"bank\", \"account\", \"portfolio\", \"Elon\", \"Musk\", \"shibaa\",\n",
    "              \"profit\", \"cro\", \"€\", \"inr\", \"mill\", \"cdc\", \"tbh\", \"hi\", \"hey\", \"plz\", \"wbu\", \"%\",\n",
    "              \"crypto.com\", \"email\", \"usdt\", \"cent\", \"ct\", \"mil\", \"ppl\", \"btc\", \"curr\"]\n",
    "    \n",
    "    correct_words = list(set(words.words()).union(set(slangs)))    \n",
    "    dataframe[\"Messages\"] = dataframe[\"Messages\"].progress_apply(lambda txt: word_in_english(txt, correct_words))\n",
    "    return dataframe[dataframe['Messages'] > \"\"]\n",
    "\n",
    "def remove_stop_words(dataframe): \n",
    "\n",
    "    def Remove_Stopwords(message, stop_words_list):\n",
    "        tokens = message.split(\" \")\n",
    "        clean_message = [word for word in tokens if not word in stop_words_list]\n",
    "        return [(\" \").join(clean_message)]\n",
    "\n",
    "    tqdm.pandas()\n",
    "    print(\"Removing Stop Words...\")\n",
    "    deselect_stop_words = ['not', 'nor', 'no', 'against', 'don', \"don't\", \n",
    "          'should', \"should've\", 'aren', \"aren't\", 'couldn', \n",
    "          \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", \n",
    "          'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", \n",
    "          'isn', \"isn't\", 'mightn', \"mightn't\", 'mustn', \"mustn't\", \n",
    "          'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", \n",
    "          'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", \n",
    "          'wouldn', \"wouldn't\"]\n",
    "\n",
    "    stop_words_list = set([stopwords.words('english').remove(word) for word in deselect_stop_words])\n",
    "    dataframe['Messages'] = dataframe['Messages'].progress_apply(lambda txt: Remove_Stopwords(txt, stop_words_list))\n",
    "    return dataframe\n",
    "\n",
    "def lemmatization(dataframe): \n",
    "    def lemmatize(message):\n",
    "        word_lemma = WordNetLemmatizer()\n",
    "        lemmatize_word = [word_lemma.lemmatize(word) for word in message]\n",
    "        return (\" \").join(lemmatize_word)\n",
    "    \n",
    "    tqdm.pandas()\n",
    "    print(\"Lemmatizing Words in Messages...\")\n",
    "    dataframe['Messages']= dataframe['Messages'].progress_apply(lambda txt: lemmatize(txt))\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding Emojis in Text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 3061/3061 [00:01<00:00, 2320.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting Accented Characters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 3061/3061 [00:00<00:00, 52919.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing Case Sensitive Characters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 3061/3061 [00:00<00:00, 306846.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing HTMLs and URLs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 3061/3061 [00:00<00:00, 307096.05it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 3061/3061 [00:01<00:00, 2887.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing Extra Whitespaces...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 3061/3061 [00:00<00:00, 47217.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanding Contractions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 3061/3061 [00:00<00:00, 36108.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing Stop Words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 3061/3061 [00:00<00:00, 127874.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatizing Words in Messages...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 3061/3061 [00:02<00:00, 1361.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking English Messages...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3061/3061 [02:44<00:00, 18.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Spelling Corrections...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|█████▉                                                                       | 227/2954 [04:47<1:53:56,  2.51s/it]"
     ]
    }
   ],
   "source": [
    "df_pre = preprocessing(df)\n",
    "df_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre.to_csv(r\"Preprocessed_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
