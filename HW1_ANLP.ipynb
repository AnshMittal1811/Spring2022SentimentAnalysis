{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2qdkjCpHgNjI",
    "outputId": "527b8bf6-cebc-4edf-e999-765048fd56cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3: can't open file 'version': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python version;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8xJsrLd9jcw0",
    "outputId": "928a0393-d900-4e06-ce97-a34149cb6cf8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "!pip install contractions\n",
    "# import contractions\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7-iIHrhtjieq",
    "outputId": "f3c243ef-4684-4a48-db20-9112d65104f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FM59-IDor3zh",
    "outputId": "bf1bbd92-a376-4301-fd73-9051f6ce73d4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (1,4,7,8,9,15,16,17,18,19,20,21,22,23,24,25,26,27,28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df= pd.read_csv(\"/content/drive/MyDrive/NLP/ANLP_HW1_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HpvVbejrtUGU"
   },
   "outputs": [],
   "source": [
    "####ONLY KEEPING'REVIEW BODY' AND 'STAR_RATING' AS DEFINED IN THE PROBLEM STATEMENT######################\n",
    "dataset = df[['star_rating','review_body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ty7ANEcqPv0K",
    "outputId": "5b437273-ab07-4e7e-dc0d-292d203d5c15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics- Positive Reviews: 733804 , Negative Reviews: 107682 , Neutral_Reviews: 60259\n"
     ]
    }
   ],
   "source": [
    "######################################DISPLAYING STATISTICS OF ALL THREE CLASSES##################################\n",
    "Positive_Reviews = dataset[dataset.star_rating ==4].shape[0] + dataset[dataset.star_rating ==5].shape[0]\n",
    "Negative_Reviews =  dataset[dataset.star_rating ==1].shape[0] + dataset[dataset.star_rating ==2].shape[0]\n",
    "Neutral_Reviews = dataset[dataset.star_rating ==3].shape[0]\n",
    "print('Statistics- Positive Reviews:', Positive_Reviews , ', Negative Reviews:', Negative_Reviews, ', Neutral_Reviews:', Neutral_Reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CfbRbGF6tUZV"
   },
   "outputs": [],
   "source": [
    "##################DISCARDING THE REVIEWS WHICH HAVE 'STAR_RATING' AS 3 I.E NEUTRAL REVIEWS###################\n",
    "dataset = dataset[dataset.star_rating != 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ox9231FBtlGg"
   },
   "outputs": [],
   "source": [
    "#####################ASSIGNING LABELS TO THE DATA; 1 CORRESPONDS TO RATINGS {4,5}, 0 CORRESPONDS TO RATINGS {1,2}#############\n",
    "dataset['star_rating'] = pd.to_numeric(dataset['star_rating'],errors='coerce')\n",
    "dataset['labels'] = np.where(dataset['star_rating']>=4, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-KhNNumtliE"
   },
   "outputs": [],
   "source": [
    "#######################DIVIDING POSITIVE & NEGATIVE REVIEWS AND SELECTING A RANDOM SAMPLE OF 10000 DATAROWS###############################\n",
    "dataset_pos= dataset.loc[dataset['labels'] == 1]\n",
    "dataset_pos= dataset.sample(100000)\n",
    "dataset_neg= dataset.loc[dataset['labels'] == 0]\n",
    "dataset_neg = dataset.sample(100000)\n",
    "dataset= dataset_pos\n",
    "dataset= dataset.append(dataset_neg)\n",
    "dataset = dataset.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UuGHBf1wFZJl",
    "outputId": "4d62699d-175a-4335-88bc-9bd67852ae62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Length of Reviews before Data Cleaning 187.05533\n"
     ]
    }
   ],
   "source": [
    "####################DISPLAYING AVERAGE LENGTH OF REVIEWS BEFORE DATA CLEANING###################\n",
    "Before_DataCleaning = dataset['review_body'].astype(str).apply(lambda x : len(str(x))).mean()\n",
    "print('Average Length of Reviews before Data Cleaning',Before_DataCleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DXx15onxt_fT"
   },
   "outputs": [],
   "source": [
    "#######################CONVERT ALL REVIEWS INTO LOWER CASE###########\n",
    "dataset['review_body']= dataset['review_body'].apply(lambda txt: str(txt).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6kdppSSt_oc"
   },
   "outputs": [],
   "source": [
    "##########################REMOVE HTML and URLs FROM CUSTOMER REVIEWS#################\n",
    "###########1. REMOVING URL's####################\n",
    "dataset['review_body']= dataset['review_body'].apply(lambda txt: re.sub(r\"http\\S+\", \"\", txt) )\n",
    "##########2. REMOVING HTML####################\n",
    "dataset['review_body']= dataset['review_body'].apply(lambda txt: BeautifulSoup(txt, 'lxml').get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Kh6Iufut_p-"
   },
   "outputs": [],
   "source": [
    "##########################REMOVE NON-ALPHABETICAL CHARACTERS#######################\n",
    "dataset['review_body']= dataset['review_body'].apply(lambda txt: re.sub(\"[^a-zA-Z]+\", \" \",txt) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yE1kKjfyt_vG"
   },
   "outputs": [],
   "source": [
    "#########################REMOVE THE EXTRA SPACES BETWEEN WORDS###################\n",
    "dataset['review_body']= dataset['review_body'].apply(lambda txt: re.sub(\" +\",\" \", txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tkwrGbRUuZnA"
   },
   "outputs": [],
   "source": [
    "########################PERFORM CONTRACTIONS#######################################\n",
    "import contractions\n",
    "dataset['review_body']= dataset['review_body'].apply(lambda txt: contractions.fix(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4D-2CQMiUhB_",
    "outputId": "a33d0cf3-1ad2-4e10-cf0f-42e27c5b42fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Length of Reviews after Data Cleaning 179.510425\n",
      "Average Length of Reviews before Pre-Processing 179.510425\n"
     ]
    }
   ],
   "source": [
    "######################DISPLAYING AVERAGE LENGTH OF REVIEWS AFTER DATA CLEANING & BEFORE PRE PROCESSING#############\n",
    "AFter_DataCleaning = dataset['review_body'].apply(len).mean()\n",
    "print('Average Length of Reviews after Data Cleaning',AFter_DataCleaning)\n",
    "print('Average Length of Reviews before Pre-Processing',AFter_DataCleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7uhZtzknuZt7",
    "outputId": "2306ffbe-982c-44b6-ac48-8d09caa19090"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#####################REMOVE STOPWORDS##########################################\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "def Remove_Stopwords(message):\n",
    "  tokens = message.split(\" \")\n",
    "  stop_words_list = set (stopwords.words('english'))\n",
    "  clean_message = [word for word in tokens if not word in stop_words_list]\n",
    "  return [(\" \").join(clean_message)]\n",
    "  \n",
    "dataset['review_body']= dataset['review_body'].apply(lambda txt: Remove_Stopwords(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OMNiWSoP1ahB"
   },
   "outputs": [],
   "source": [
    "######################################PERFORM LEMMATIZATION#############################\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def lemmatize(message):\n",
    "    word_lemma = WordNetLemmatizer()\n",
    "    lemmatize_word= [word_lemma.lemmatize(word) for word in message]\n",
    "    return (\" \").join(lemmatize_word)\n",
    "dataset['review_body']= dataset['review_body'].apply(lambda txt: lemmatize(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kOTOgU-PSukd",
    "outputId": "4a87a817-0555-41ea-ec0e-698a9a091068"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Length of Reviews after Pre-Processing 113.981\n"
     ]
    }
   ],
   "source": [
    "######################DISPLAYING AVERAGE LENGTH OF REVIEWS AFTER PRE PROCESSING#############\n",
    "AFter_Preprocessing = dataset['review_body'].apply(len).mean()\n",
    "print('Average Length of Reviews after Pre-Processing',AFter_Preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZzfkIo3j4PuP"
   },
   "outputs": [],
   "source": [
    "############################################FEATURE EXTRACTION & SPLITTING DATA FOR TRAINING & TESTING######################\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "data_train, data_test, y_train, y_test = train_test_split(dataset['review_body'], dataset['labels'], test_size = 0.2)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "Training_data = tfidf.fit_transform(data_train)\n",
    "Testing_data = tfidf.transform(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CQlxBhs04gvx",
    "outputId": "d9f7e4b4-5fc7-4361-dae1-370790616c15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Model(Testing_data)--Accuracy: 0.919625\n",
      "Perceptron Model(Testing_data)--Precision Score: 0.9516885958598451\n",
      "Perceptron Model(Testing_data)--Recall Score: 0.9555677613320999\n",
      "Perceptron Model(Testing_data)--F1 Score: 0.9536242336819329\n",
      "Perceptron Model(Training_data)--Accuracy: 0.9480125\n",
      "Perceptron Model(Training_data)--Precision Score: 0.9686538225902942\n",
      "Perceptron Model(Training_data)--Recall Score: 0.9713141709007596\n",
      "Perceptron Model(Training_data)--F1 Score: 0.9699821726295733\n"
     ]
    }
   ],
   "source": [
    "######################################PERCEPTRON MODEL##################################\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "Perceptron_Model= Perceptron()\n",
    "Perceptron_Model.fit(Training_data,y_train)\n",
    "Prediction = Perceptron_Model.predict(Testing_data)\n",
    "print('Perceptron Model(Testing_data)--Accuracy:',accuracy_score(y_test,Prediction))\n",
    "print('Perceptron Model(Testing_data)--Precision Score:',precision_score(y_test,Prediction))\n",
    "print('Perceptron Model(Testing_data)--Recall Score:',recall_score(y_test,Prediction))\n",
    "print('Perceptron Model(Testing_data)--F1 Score:',f1_score(y_test,Prediction))\n",
    "Prediction = Perceptron_Model.predict(Training_data)\n",
    "print('Perceptron Model(Training_data)--Accuracy:',accuracy_score(y_train,Prediction))\n",
    "print('Perceptron Model(Training_data)--Precision Score:',precision_score(y_train,Prediction))\n",
    "print('Perceptron Model(Training_data)--Recall Score:',recall_score(y_train,Prediction))\n",
    "print('Perceptron Model(Training_data)--F1 Score:',f1_score(y_train,Prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vci07v068NWg",
    "outputId": "d57e4e60-29e0-4008-9a71-247fb7dbd370"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model(Testing_data)--Accuracy: 0.850925\n",
      "SVM Model(Testing_data)--Precision Score: 0.900708227192565\n",
      "SVM Model(Testing_data)--Recall Score: 0.9301572617946346\n",
      "SVM Model(Testing_data)--F1 Score: 0.9151959041456303\n",
      "SVM Model(Training_data)--Accuracy: 0.8515875\n",
      "SVM Model(Training_data)--Precision Score: 0.9015295362345243\n",
      "SVM Model(Training_data)--Recall Score: 0.9299513591257652\n",
      "SVM Model(Training_data)--F1 Score: 0.915519915754721\n"
     ]
    }
   ],
   "source": [
    "############################################SUPPORT VECTOR MACHINE#######################\n",
    "from sklearn.svm import SVC\n",
    "svm_Model = SVC(kernel='linear',max_iter=500)\n",
    "svm_Model.fit(Training_data,y_train)\n",
    "prediction = svm_Model.predict(Testing_data)\n",
    "print('SVM Model(Testing_data)--Accuracy:',accuracy_score(y_test,prediction))\n",
    "print('SVM Model(Testing_data)--Precision Score:',precision_score(y_test,prediction))\n",
    "print('SVM Model(Testing_data)--Recall Score:',recall_score(y_test,prediction))\n",
    "print('SVM Model(Testing_data)--F1 Score:',f1_score(y_test,prediction))\n",
    "prediction = svm_Model.predict(Training_data)\n",
    "print('SVM Model(Training_data)--Accuracy:',accuracy_score(y_train,prediction))\n",
    "print('SVM Model(Training_data)--Precision Score:',precision_score(y_train,prediction))\n",
    "print('SVM Model(Training_data)--Recall Score:',recall_score(y_train,prediction))\n",
    "print('SVM Model(Training_data)--F1 Score:',f1_score(y_train,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JqRBwYIcMzTT",
    "outputId": "8a5fb1b8-a417-4803-aa0e-6757402f3a0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic_Regression(Testing_data) Model--Accuracy: 0.9397\n",
      "Logistic_Regression(Testing_data) Model--Precision Score: 0.9481394830659536\n",
      "Logistic_Regression(Testing_data) Model--Recall Score: 0.9841003700277521\n",
      "Logistic_Regression(Testing_data) Model--F1 Score: 0.9657852927825692\n",
      "Logistic_Regression(Training_data) Model--Accuracy: 0.94645\n",
      "Logistic_Regression(Training_data) Model--Precision Score: 0.9527736498036015\n",
      "Logistic_Regression(Training_data) Model--Recall Score: 0.9869977811666583\n",
      "Logistic_Regression(Training_data) Model--F1 Score: 0.9695838007469151\n"
     ]
    }
   ],
   "source": [
    "#########################################LOGISTIC REGRESSION##############################\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg_Model = LogisticRegression(max_iter = 500)\n",
    "logreg_Model.fit(Training_data, y_train)\n",
    "prediction = logreg_Model.predict(Testing_data)\n",
    "print('Logistic_Regression(Testing_data) Model--Accuracy:',accuracy_score(y_test,prediction))\n",
    "print('Logistic_Regression(Testing_data) Model--Precision Score:',precision_score(y_test,prediction))\n",
    "print('Logistic_Regression(Testing_data) Model--Recall Score:',recall_score(y_test,prediction))\n",
    "print('Logistic_Regression(Testing_data) Model--F1 Score:',f1_score(y_test,prediction))\n",
    "prediction = logreg_Model.predict(Training_data)\n",
    "print('Logistic_Regression(Training_data) Model--Accuracy:',accuracy_score(y_train,prediction))\n",
    "print('Logistic_Regression(Training_data) Model--Precision Score:',precision_score(y_train,prediction))\n",
    "print('Logistic_Regression(Training_data) Model--Recall Score:',recall_score(y_train,prediction))\n",
    "print('Logistic_Regression(Training_data) Model--F1 Score:',f1_score(y_train,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Hs85aCuNmoy",
    "outputId": "734dedb2-1d74-4b30-8479-c09d2eca11a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinominal_Naive_Bayes(Testing_data) Model--Accuracy: 0.8843\n",
      "Multinominal_Naive_Bayes(Testing_data) Model--Precision Score: 0.887351983039454\n",
      "Multinominal_Naive_Bayes(Testing_data) Model--Recall Score: 0.9921658186864015\n",
      "Multinominal_Naive_Bayes(Testing_data) Model--F1 Score: 0.9368363586733999\n",
      "Multinominal_Naive_Bayes(Training_data) Model--Accuracy: 0.886825\n",
      "Multinominal_Naive_Bayes(Training_data) Model--Precision Score: 0.8899077214393639\n",
      "Multinominal_Naive_Bayes(Training_data) Model--Recall Score: 0.9918257312392943\n",
      "Multinominal_Naive_Bayes(Training_data) Model--F1 Score: 0.9381066965628503\n"
     ]
    }
   ],
   "source": [
    "######################################MULTINOMINAL REGRESSION#############################\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "MulNb_Model = MultinomialNB(alpha=6.5, fit_prior=False)\n",
    "MulNb_Model.fit(Training_data,y_train)\n",
    "prediction = MulNb_Model.predict(Testing_data)\n",
    "print('Multinominal_Naive_Bayes(Testing_data) Model--Accuracy:',accuracy_score(y_test,prediction))\n",
    "print('Multinominal_Naive_Bayes(Testing_data) Model--Precision Score:',precision_score(y_test,prediction))\n",
    "print('Multinominal_Naive_Bayes(Testing_data) Model--Recall Score:',recall_score(y_test,prediction))\n",
    "print('Multinominal_Naive_Bayes(Testing_data) Model--F1 Score:',f1_score(y_test,prediction))\n",
    "\n",
    "prediction = MulNb_Model.predict(Training_data)\n",
    "print('Multinominal_Naive_Bayes(Training_data) Model--Accuracy:',accuracy_score(y_train,prediction))\n",
    "print('Multinominal_Naive_Bayes(Training_data) Model--Precision Score:',precision_score(y_train,prediction))\n",
    "print('Multinominal_Naive_Bayes(Training_data) Model--Recall Score:',recall_score(y_train,prediction))\n",
    "print('Multinominal_Naive_Bayes(Training_data) Model--F1 Score:',f1_score(y_train,prediction))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW1_ANLP_AnmolSajnani_2581081096.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
